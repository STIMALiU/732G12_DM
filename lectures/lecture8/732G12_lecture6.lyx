#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass beamer
\begin_preamble
%\usetheme{Warsaw}
\usetheme{Boadilla}
% or ...

\usecolortheme{orchid}
\setbeamertemplate{footline}[text line]{} % makes the footer EMPTY

\setbeamercovered{transparent}
% or whatever (possibly just delete it)
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}
\end_preamble
\use_default_options false
\begin_modules
algorithm2e
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "times" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
This file is a solution template for:
\end_layout

\begin_layout Itemize
Talk at a conference/colloquium.
 
\end_layout

\begin_layout Itemize
Talk length is about 20min.
 
\end_layout

\begin_layout Itemize
Style is ornate.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Copyright 2004 by Till Tantau <tantau@users.sourceforge.net>.
 
\end_layout

\begin_layout Plain Layout
In principle, this file can be redistributed and/or modified under the terms
 of the GNU Public License, version 2.
 However, this file is supposed to be a template to be modified for your
 own needs.
 For this reason, if you use this file as a template and not specifically
 distribute it as part of a another package/program, the author grants the
 extra permission to freely copy and modify this file as you see fit and
 even to delete this copyright notice.
 
\end_layout

\end_inset


\end_layout

\begin_layout Title
Föreläsning 6 - Neurala Nätverk
\begin_inset Argument 1
status collapsed

\begin_layout Plain Layout
Short Paper Title
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
optional, use only with long paper titles
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Author
Josef Wilzen
\end_layout

\begin_layout Date
2022-09-12
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
If you have a file called "institution-logo-filename.xxx", where xxx is a
 graphic format that can be processed by latex or pdflatex, resp., then you
 can add a logo by uncommenting the following:
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

%
\backslash
pgfdeclareimage[height=0.5cm]{institution-logo}{institution-logo-filename}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

%
\backslash
logo{
\backslash
pgfuseimage{institution-logo}}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
The following causes the table of contents to be shown at the beginning
 of every subsection.
 Delete this, if you do not want it.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
AtBeginSubsection[]{
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

  
\backslash
frame<beamer>{ 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

    
\backslash
frametitle{Outline}   
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

    
\backslash
tableofcontents[currentsection,currentsubsection] 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

  }
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
If you wish to uncover everything in a step-wise fashion, uncomment the
 following command:
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

%
\backslash
beamerdefaultoverlayspecification{<+->}
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Outline
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Structuring a talk is a difficult task and the following structure may not
 be suitable.
 Here are some rules that apply for this solution: 
\end_layout

\begin_layout Itemize
Exactly two or three sections (other than the summary).
 
\end_layout

\begin_layout Itemize
At *most* three subsections per section.
 
\end_layout

\begin_layout Itemize
Talk about 30s to 2min per frame.
 So there should be between about 15 and 30 frames, all told.
\end_layout

\begin_layout Itemize
A conference audience is likely to know very little of what you are going
 to talk about.
 So *simplify*! 
\end_layout

\begin_layout Itemize
In a 20min talk, getting the main ideas across is hard enough.
 Leave out details, even if it means being less precise than you think necessary.
 
\end_layout

\begin_layout Itemize
If you omit details that are vital to the proof/implementation, just say
 so once.
 Everybody will be happy with that.
 
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
Optimering av neurala nätverk
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Intro
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard

\bar under
\begin_inset CommandInset href
LatexCommand href
name "http://playground.tensorflow.org/"
target "http://playground.tensorflow.org/"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
Lek runt med olika modeller här!
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Optimering av neurala nätverk
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient descent
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Kostnadsfunktion:
\begin_inset Formula 
\[
L\left(\theta_{n}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Gradient descent: hitta minimum på en funktion
\begin_inset Formula 
\[
\theta_{n+1}=\theta_{n}-\epsilon\cdot\nabla h\left(\theta_{n}\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
Vi behöver gradienten (partiella derivator) 
\begin_inset Formula $\nabla L\left(\theta_{n}\right)$
\end_inset


\end_layout

\begin_layout Itemize
Backpropagation: kedjeregeln för derivator på neurala nätverk
\end_layout

\begin_layout Itemize
Gradient descent: dyrt när vi har många obs!
\end_layout

\begin_layout Itemize
Vanligt i statistik: (oberoende obs)
\begin_inset Formula 
\[
L\left(\theta\right)=\sum_{i=1}^{N}L_{i}\left(f\left(x^{(i)},\theta\right),y^{(i)}\right)
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Stochastic gradient descent (SGD) 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
SGD: 
\end_layout

\begin_layout Itemize
Dyrt att beräkna 
\begin_inset Formula $\nabla L\left(\theta\right)$
\end_inset

 för alla datapunkter
\end_layout

\begin_layout Itemize
Gör en väntevärdesriktig skattning av 
\begin_inset Formula $\nabla\hat{L}\left(\theta\right)$
\end_inset

 genom att ta ett slumpmässigt sample från data (mini-batch)
\end_layout

\begin_layout Itemize
Det finns varians i 
\begin_inset Formula $\nabla\hat{L}\left(\theta\right)$
\end_inset

, större batch ger mindre varians men blir dyrare att beräkna.
\end_layout

\begin_layout Itemize
Kräver många iterationer och liten learning rate (
\begin_inset Formula $\epsilon$
\end_inset

)
\end_layout

\begin_deeper
\begin_layout Itemize
För liten: tar väldigt lång tid!
\end_layout

\begin_layout Itemize
För stor: värdet på kostnadsfunktionen varierar kraftigt!
\end_layout

\end_deeper
\begin_layout Itemize
Kräver att vi har oberoende observationer i likelihoodfunktionen.
\end_layout

\begin_layout Itemize
Väldigt vanlig inom maskininlärning
\end_layout

\begin_deeper
\begin_layout Itemize
Speciellt inom deep learning
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Stochastic gradient descent (SGD)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Learning rate 
\begin_inset Formula $\epsilon$
\end_inset

 (inlärningstakt)
\end_layout

\begin_layout Itemize
Mycket viktig parameter
\end_layout

\begin_layout Itemize
Ofta behöver vi sakta minska inlärningstakten med antalet iterationer i
 SGD
\end_layout

\begin_deeper
\begin_layout Itemize
Schema (schedule) för minskningen: 
\begin_inset Formula $\epsilon_{1},\epsilon_{2},\ldots,\epsilon_{k},\ldots$
\end_inset


\end_layout

\begin_layout Itemize
Gradienten kommer alltid att ha lite brus, även i en lokal minimipunkt
\end_layout

\begin_layout Itemize
Krav för teoretisk konvergens:
\begin_inset Formula 
\[
\sum_{k=1}^{\infty}\epsilon_{k}=\infty
\]

\end_inset


\begin_inset Formula 
\[
\sum_{k=1}^{\infty}\epsilon_{k}^{2}<\infty
\]

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Stochastic gradient descent (SGD)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Problem:
\series default
 försvinnande eller exploderande gradienter 
\begin_inset Formula $\rightarrow$
\end_inset

 i backproagation multiplicerar vi många derivator med varandra
\end_layout

\begin_deeper
\begin_layout Itemize
Om derivatorna är för nära noll: Produkten blir noll
\end_layout

\begin_layout Itemize
Om derivatorna är för stora: Produkten 
\begin_inset Formula $\rightarrow\infty$
\end_inset


\end_layout

\begin_layout Itemize
Nätverk med ReLu fungerar ofta bättre än tex sigmoid
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Stochastic gradient descent (SGD)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard

\series bold
Require
\series default
: Initial parameter 
\begin_inset Formula $\theta$
\end_inset

, Learning rate schedule: 
\begin_inset Formula $\epsilon_{1},\epsilon_{2},\ldots,\epsilon_{k},\ldots$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $k\leftarrow1$
\end_inset


\end_layout

\begin_layout Itemize

\series bold
while
\series default
 stopping criterion not met 
\series bold
do
\series default
 
\end_layout

\begin_deeper
\begin_layout Itemize
Sample a minibatch of 
\begin_inset Formula $m$
\end_inset

 examples from the training set 
\begin_inset Formula $\left(x^{(1)},\ldots,x^{(m)}\right)$
\end_inset

, with corresponding 
\begin_inset Formula $y$
\end_inset

 values.
\end_layout

\begin_layout Itemize
Compute gradient estimate:
\begin_inset Formula 
\[
\hat{g}\leftarrow\frac{1}{m}\nabla\sum_{i}L\left(f\left(x^{(i)},\theta\right),y^{(i)}\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
Apply update:
\begin_inset Formula 
\[
\theta\leftarrow\theta-\epsilon_{k}\cdot\hat{g}
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $k\leftarrow k+1$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\series bold
end while
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Momentum
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
SGD är ofta bra, men ibland för långsamt
\end_layout

\begin_layout Itemize
Momentum: metod att snabba upp och göra SGD mer effektiv
\end_layout

\begin_layout Itemize
Idé:
\end_layout

\begin_deeper
\begin_layout Itemize
Ta glidande medelvärde över elementen i gradientvektorn
\end_layout

\begin_layout Itemize
Vi vill inte ha hela 
\begin_inset Quotes eld
\end_inset

historien
\begin_inset Quotes erd
\end_inset

 med gradienter 
\begin_inset Formula $\rightarrow$
\end_inset

 exponentiellt avtagande glidande medelvärde 
\end_layout

\end_deeper
\begin_layout Itemize
Hyperparameter: 
\begin_inset Formula $\alpha$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Ju större 
\begin_inset Formula $\alpha$
\end_inset

 är i relation till 
\begin_inset Formula $\epsilon$
\end_inset

 desto mer påverkar tidigare värden på gradienten.
\end_layout

\begin_layout Itemize
Ofta sätts 
\begin_inset Formula $\alpha$
\end_inset

 till 
\begin_inset Formula $0.5$
\end_inset

, 
\begin_inset Formula $0.9$
\end_inset

 eller 
\begin_inset Formula $0.99$
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Stochastic gradient descent with momentum
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard

\series bold
Require
\series default
: Initial parameter 
\begin_inset Formula $\theta$
\end_inset

, initial velocity 
\begin_inset Formula $v$
\end_inset

, Learning rate schedule: 
\begin_inset Formula $\epsilon_{1},\epsilon_{2},\ldots,\epsilon_{k},\ldots$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $k\leftarrow1$
\end_inset


\end_layout

\begin_layout Itemize

\series bold
while
\series default
 stopping criterion not met 
\series bold
do
\series default
 
\end_layout

\begin_deeper
\begin_layout Itemize
Sample a minibatch of 
\begin_inset Formula $m$
\end_inset

 examples from the training set 
\begin_inset Formula $\left(x^{(1)},\ldots,x^{(m)}\right)$
\end_inset

, with corresponding 
\begin_inset Formula $y$
\end_inset

 values.
\end_layout

\begin_layout Itemize
Compute gradient estimate:
\begin_inset Formula 
\[
\hat{g}\leftarrow\frac{1}{m}\nabla\sum_{i}L\left(f\left(x^{(i)},\theta\right),y^{(i)}\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
Compute velocity:
\begin_inset Formula 
\[
v\leftarrow\alpha\cdot v-\epsilon_{k}\cdot\hat{g}
\]

\end_inset


\end_layout

\begin_layout Itemize
Apply update:
\begin_inset Formula 
\[
\theta\leftarrow\theta+v
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $k\leftarrow k+1$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\series bold
end while
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Momentum
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Visualisering: 
\bar under

\begin_inset CommandInset href
LatexCommand href
name "här"
target "https://distill.pub/2017/momentum/"
literal "false"

\end_inset


\bar default

\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename figs/Momentum.png
	scale 23

\end_inset


\begin_inset Newline newline
\end_inset

Från: Deep Learning, Ian Goodfellow and Yoshua Bengio and Aaron Courville,
 2016, 
\bar under

\begin_inset CommandInset href
LatexCommand href
name "länk"
target "https://www.deeplearningbook.org/"
literal "false"

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Startvärden
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Neurala nätverk: tusentals parametrar som vi behöver skatta
\end_layout

\begin_deeper
\begin_layout Itemize
SGD är iterativt: vi börjar med några 
\series bold
initiala värden
\series default
 på alla parametrar som vi sedan ändrar i varje iteration av algoritmen
\end_layout

\end_deeper
\begin_layout Itemize
Startvärden: Svårt problem!
\end_layout

\begin_layout Itemize
Dåliga startvärden kan leda till: 
\end_layout

\begin_deeper
\begin_layout Itemize
SGD konvergerar inte till en lösning (lokalt optima)
\end_layout

\begin_layout Itemize
Numeriska problem som gör att beräkningarna inte går att utföra
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Startvärden
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Startvärden
\end_layout

\begin_layout Itemize
Bryta symmetri mellen noderna: vikterna mellan två lager kan inte start
 på exakt samma värden
\end_layout

\begin_layout Itemize
Vikter brukar sättas till slumpmässiga tal, som är relativt nära noll
\end_layout

\begin_deeper
\begin_layout Itemize
Uniformfördelning
\end_layout

\begin_layout Itemize
Normalfördelning
\end_layout

\end_deeper
\begin_layout Itemize
Bias: brukar sättas till något värde
\end_layout

\begin_layout Itemize
Hur vi specificerar startvärden kan ses som en hyperparameter
\end_layout

\begin_layout Itemize
Se dok för Keras: 
\bar under

\begin_inset CommandInset href
LatexCommand href
name "länk"
target "https://keras.rstudio.com/reference/index.html"
literal "false"

\end_inset


\bar default
, under Initializers.
\end_layout

\begin_layout Itemize
Svårt område att forska på
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Adaptiv inlärningstakt
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Kan vara svårt att specificera en learning rate som passar alla våra parametrar
 i ett neuralt nätverk när vi använder SGD
\end_layout

\begin_layout Standard
Algoritmer med adaptiv learning rate:
\end_layout

\begin_layout Itemize
RMSProp
\end_layout

\begin_deeper
\begin_layout Itemize
Skala om gradienten beroende på träningshistoriken, gör att vi:
\end_layout

\begin_deeper
\begin_layout Itemize
tar större steg för de parametrar som har små derivator: mer flacka områden,
 som vi vill passera snabbare
\end_layout

\begin_layout Itemize
tar mindre steg för de parametrar som har stora derivator: här vill vi vara
 mer försiktiga
\end_layout

\begin_layout Itemize
Skala med exponentiellt avtagande glidande medelvärde för tidigare gradientvärde
n (elementvis)
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Adam
\end_layout

\begin_deeper
\begin_layout Itemize
Kan ses som en blanding av RMSProp och momentum
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
RMSProp
\series default
 och 
\series bold
Adam
\series default
 är vanliga standardval av optimeringsmetoder inom deep learning
\end_layout

\begin_layout Itemize
Båda har några hyperparametrar
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Adam och RMSProp
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Graphics
	filename figs/DL_training_curves.png
	scale 30

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
Regularisering
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Regularisering
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Regularisering genom normstraff
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Vilken kostnadsfunktion har Ridge regession?
\end_layout

\begin_layout Itemize
Vilken kostnadsfunktion har Lasso regession?
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Regularisering genom normstraff
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Ridge regession:
\end_layout

\begin_layout Standard
Ridge= OLS + 
\begin_inset Formula $l^{2}$
\end_inset

-norm på 
\begin_inset Formula $\beta$
\end_inset

:
\begin_inset Formula 
\[
\underset{\beta}{arg\,min}\,L\left(\beta\right)=\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p}\beta_{j}x_{i,j}\right)^{2}+\lambda\sum_{j=1}^{p}\beta_{j}^{2}\qquad\lambda\ge0
\]

\end_inset

Lasso regression:
\end_layout

\begin_layout Standard
Lasso= OLS + 
\begin_inset Formula $l^{1}$
\end_inset

-norm på 
\begin_inset Formula $\beta$
\end_inset

:
\begin_inset Formula 
\[
\underset{\beta}{arg\,min}\,L\left(\beta\right)=\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p}\beta_{j}x_{i,j}\right)^{2}+\lambda\sum_{j=1}^{p}\left|\beta_{j}\right|\qquad\lambda\ge0
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Neurala nätverk + normstraff
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Linjär regression: 
\end_layout

\begin_layout Itemize
Notera: Ridge/Lasso tvingar parameterarna att vara relativt nära noll 
\begin_inset Formula $\rightarrow$
\end_inset

 detta gör att de funktionen de anpassar blir enklare
\end_layout

\begin_layout Standard
Vi kan använda samma idé inom deep learning/neurala nätverk!
\end_layout

\begin_layout Itemize
Genom att tvinga parameterarna att vara nära noll så tvingas nätverket att
 skapa en enklare funktion
\end_layout

\begin_layout Itemize
Detta minskar överanpassning
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Neurala nätverk + normstraff
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Generellt:
\begin_inset Formula 
\[
\tilde{h}\left(\theta|X,y\right)=h\left(\theta|X,y\right)+\alpha\Omega\left(\theta\right)
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $h\left(\theta|X,y\right)$
\end_inset

: vanliga kostnadsfunktionen
\end_layout

\begin_layout Itemize
\begin_inset Formula $\Omega\left(\theta\right)$
\end_inset

 straffterm baserad på vektor/matrisnorm, dvs 
\begin_inset Formula $l^{2}$
\end_inset

-norm eller 
\begin_inset Formula $l^{1}$
\end_inset

-norm.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\alpha$
\end_inset

: hyperparameter, med 
\begin_inset Formula $\alpha\ge0$
\end_inset


\end_layout

\begin_layout Itemize
Vad innebär stora värden på 
\begin_inset Formula $\alpha$
\end_inset

? Små?
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Neurala nätverk + normstraff
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Använder Frobenius normen:
\begin_inset Formula 
\[
||A||_{F}^{2}=\sum_{i=1}^{q}\sum_{j=1}^{p}a_{ij}^{2}\qquad||A||_{F}^{1}=\sum_{i=1}^{q}\sum_{j=1}^{p}|a_{ij}|
\]

\end_inset

där 
\begin_inset Formula $A$
\end_inset

 är en 
\begin_inset Formula $p\times q$
\end_inset

 matris
\end_layout

\begin_layout Itemize
Låt 
\begin_inset Formula $W_{l}$
\end_inset

 bara en matris med vikter mellan lager 
\begin_inset Formula $l$
\end_inset

 och 
\begin_inset Formula $l+1$
\end_inset


\end_layout

\begin_layout Itemize
Kostnadsfunktioner:
\begin_inset Formula 
\[
\tilde{h}\left(\theta|X,y\right)=h\left(\theta|X,y\right)+\frac{\alpha}{2}\sum_{l=1}^{h}||W_{l}||_{F}^{2}
\]

\end_inset


\begin_inset Formula 
\[
\tilde{h}\left(\theta|X,y\right)=h\left(\theta|X,y\right)+\frac{\alpha}{2}\sum_{l=1}^{h}||W_{l}||_{F}^{1}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Neurala nätverk + normstraff
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Vi kan ha olika 
\begin_inset Formula $\alpha$
\end_inset

 för olika lager
\begin_inset Formula 
\[
\frac{1}{2}\sum_{l=1}^{h}\alpha_{l}||W_{l}||_{F}^{2}
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $||W_{l}||_{F}^{2}$
\end_inset

 kallas ibland för weight decay (tvingar vikterna att bli mindre)
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Dropout
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
I varje iteration när vi tränar vår modell med SGD:
\end_layout

\begin_deeper
\begin_layout Itemize
låt varje nod vara noll med en viss sannolikhet
\end_layout

\begin_layout Itemize
Skapa indikatorfunktion för varje nod i de lager med dropout: 
\begin_inset Formula $I_{i}\sim p\left(I_{i}=1\right)=r$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Nätverket tvingas att inte 
\begin_inset Quotes eld
\end_inset

lita på
\begin_inset Quotes erd
\end_inset

 specifika kopplingar mellan lager
\end_layout

\begin_layout Itemize
Nätverket måste distribuera sin kapacitet över hela nätverket, vilket motverkar
 överanpassning
\end_layout

\begin_layout Itemize
När värdena på vikterna sprids ut över många kopplingar så blir de mindre
\end_layout

\begin_deeper
\begin_layout Itemize
Liknande effekt som vid normstraff.
\end_layout

\end_deeper
\begin_layout Itemize
Dropout används vid träning, inte vid testning
\end_layout

\begin_layout Itemize
Notera likheten med Random forest
\end_layout

\end_deeper
\begin_layout Frame

\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Dropout
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Graphics
	filename figs/dropout_example.png
	scale 25

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Mer regularisering
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Det flera andra sätt att regularisera:
\end_layout

\begin_layout Itemize
Skaffa mer träningsdata!
\end_layout

\begin_layout Itemize
Early stopping: Stanna träningen innan maximala antal iterationer är uppnått.
 
\end_layout

\begin_deeper
\begin_layout Itemize
Ofta används ett valideringsdata för att avgöra när det är dags att stanna
 skattningen
\end_layout

\end_deeper
\begin_layout Itemize
Dataset Augmentation: Skapa mer artificiell data att träna på
\end_layout

\begin_deeper
\begin_layout Itemize
Ofta i samband med bilddata
\end_layout

\begin_layout Itemize
Speglingar, rotationer, beskärningar, addera brus mm
\end_layout

\end_deeper
\begin_layout Itemize
Ensamplemetoder
\end_layout

\begin_deeper
\begin_layout Itemize
Generera flera uppsättningar med startvärden och träna flera modeller: aggregera
 resultatet vid testdata
\end_layout

\end_deeper
\begin_layout Itemize
Parameter Tying and Parameter Sharing
\end_layout

\begin_deeper
\begin_layout Itemize
Sätt en viss struktur på arkitekturen som tvingar vissa kopplingar att vara
 identiska/glesa mm 
\begin_inset Formula $\rightarrow$
\end_inset

 CNN
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Specialla typer av modeller
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Residual nets: Kopplingar som hoppar över lager
\end_layout

\begin_deeper
\begin_layout Itemize
Skip layer connections
\end_layout

\begin_layout Itemize
Tänk: 
\begin_inset Quotes eld
\end_inset

modellera residualerna på en linjär modell med ett neuralt nätverk
\begin_inset Quotes erd
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Transfer learning:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

Överföra kunskap mellan olika problem/uppgifter
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Itemize
Finns olika sorter, vanlig variant: Använda förtränade nätverk (pretrained
 models) på andra problem.
 
\end_layout

\begin_deeper
\begin_layout Itemize
Ta ett neuralt nätverk med skattade vikter som används till ett dataset/problem
\end_layout

\begin_layout Itemize
Ta modellen och starta optimieringen från förtränade vikterna på ett 
\series bold
annat
\series default
 dataset
\end_layout

\begin_layout Itemize
Kan ses som ett avancerat sätt att specificera startvärden till optimiering
\end_layout

\begin_layout Itemize
Är ofta effektivt på mindre dataset
\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
Faltade nätverk
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Faltade nätverk
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Faltade nätverk
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Faltade nätverk: Convolutional networks, convolutional neural networks,
 ConvNet, CNN
\end_layout

\begin_layout Itemize
Specialla nätverk som passar för data som har en rutnätsstruktur (grid):
\end_layout

\begin_deeper
\begin_layout Itemize
1-D nät: tidserier
\end_layout

\begin_layout Itemize
2-D nät: bilder (pixlar)
\end_layout

\begin_layout Itemize
Tänk pixlar organiserade i en matris = rutnät
\end_layout

\begin_layout Itemize

\series bold
Mycket framgångsrik modellklass!!!!!
\end_layout

\begin_layout Itemize
Revolutionerade bildbehandlingen 2012
\end_layout

\begin_layout Itemize
Lite historia här: 
\begin_inset CommandInset href
LatexCommand href
name "länk"
target "https://dataconomy.com/2017/04/history-neural-networks/"
literal "false"

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Använder faltning mellan minst två lager i nätverket, istället för vanlig
 matrismultiplikation.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Faltning
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset CommandInset href
LatexCommand href
name "Faltning"
target "https://sv.wikipedia.org/wiki/Faltning"
literal "false"

\end_inset

 är en speciell typ av linjär operator
\end_layout

\begin_layout Itemize
Kan defineras lite olika inom olika fält
\begin_inset Formula 
\[
y\left(t\right)=\int x\left(\tau\right)h\left(t-\tau\right)d\tau=\int x\left(t-\tau\right)h\left(\tau\right)d\tau
\]

\end_inset


\begin_inset Formula 
\[
y\left(t\right)=\left(x*h\right)\left(t\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
Kan ses som en sammanviktning av två kontinuerliga funktioner 
\begin_inset Formula $x\text{\left(t\right)}$
\end_inset

 och 
\begin_inset Formula $h\left(t\right)$
\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $x\text{\left(t\right)}$
\end_inset

: insignal (input) och 
\begin_inset Formula $h\left(t\right)$
\end_inset

 kärnfunktion (kernel, kernel funciton) eller filter
\end_layout

\begin_layout Itemize
\begin_inset Formula $y\left(t\right)$
\end_inset

: utsingal, inom DL: 
\series bold
feature map
\end_layout

\end_deeper
\begin_layout Itemize
Se exempel 
\begin_inset CommandInset href
LatexCommand href
name "här"
target "https://en.wikipedia.org/wiki/Convolution#/media/File:Convolution_of_spiky_function_with_box2.gif"
literal "false"

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Faltning
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Vanlig inom statistik, matematik, singalbehandling mm
\end_layout

\begin_deeper
\begin_layout Itemize
Används för att filtrera signaler/tidserier, tex för att ta bort brus
\end_layout

\begin_layout Itemize
Används bla när vi vill beräkna täthetsfunktionen för 
\begin_inset CommandInset href
LatexCommand href
name "summan av två oberoende slumpvariabler"
target "https://en.wikipedia.org/wiki/Convolution_of_probability_distributions"
literal "false"

\end_inset


\bar under

\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $p\left(X\right),p\left(Y\right)$
\end_inset

, 
\begin_inset Formula $X+Y=Z$
\end_inset

, vad är 
\begin_inset Formula $p\left(Z\right)$
\end_inset

?
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Faltning
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Om vi har signaler som är samplade i diskret tid
\begin_inset Formula 
\[
y\left(t\right)=\left(x*h\right)\left(t\right)=\sum_{\tau=-\infty}^{\infty}x\left(\tau\right)h\left(t-\tau\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $h\left(t\right)$
\end_inset

 är här en vektor
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Faltning
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
2D: 
\begin_inset Formula 
\[
y\left(i,j\right)=\left(x*h\right)\left(i,j\right)=\sum_{m}\sum_{n}x\left(m,n\right)h\left(i-m,j-n\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
Kan generaliseras till högre dimensioner
\end_layout

\begin_deeper
\begin_layout Itemize
tensor = en matris/tabell med mer än 2 dimensioner, tänk 
\family typewriter
array()
\family default
 i R.
\end_layout

\begin_layout Itemize
TensorFlow
\end_layout

\end_deeper
\begin_layout Itemize
Vi kan använda faltning som ett sätt att koppla ihop två lager
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Varför faltning?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Glesa kopplingar
\end_layout

\begin_layout Itemize
Spatial information: närliggande pixlar relaterar till varandra mer än pixlar
 långt borta
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename figs/CNN_sparse_con.png
	scale 15

\end_inset


\end_layout

\begin_layout Standard

\size scriptsize
Från: Deep Learning, Ian Goodfellow and Yoshua Bengio and Aaron Courville,
 2016, 
\begin_inset CommandInset href
LatexCommand href
name "länk"
target "https://www.deeplearningbook.org/"
literal "false"

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Klassificering av bilder: 2D
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
ImageNet dataset / challenge, 1.2 million training images, 1000 classes
\end_layout

\begin_layout Itemize
\begin_inset CommandInset href
LatexCommand href
name "http://www.image-net.org"
target "http://www.image-net.org"
literal "false"

\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename figs/imageNet.png
	scale 25

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset CommandInset href
LatexCommand href
name "https://devopedia.org/imagenet"
target "https://devopedia.org/imagenet"
literal "false"

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Klassificering av bilder: 2D
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard

\series bold
\begin_inset Graphics
	filename figs/Chexnet.png
	scale 30

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Bildanalys/Datorseende
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Stora dataset
\end_layout

\begin_layout Itemize
Lätt att dela data
\end_layout

\begin_layout Itemize
2D/3D data
\end_layout

\begin_layout Itemize
ImageNet
\end_layout

\begin_layout Itemize
Många förtränade modeller för 2D-data
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Vad är en bild?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Bild: 2D signal, varje pixel är ljusstyrka
\end_layout

\begin_layout Itemize
Digitala bilder vanligast
\end_layout

\begin_layout Itemize
Varje pixel har 2D koordinater (x,y)
\end_layout

\begin_layout Itemize
Varje pixel kan flera olika värden
\end_layout

\begin_deeper
\begin_layout Itemize
Färgbilder: red, grön och blå 
\begin_inset Formula $\rightarrow$
\end_inset

 tre kanaler (channels)
\end_layout

\begin_layout Itemize
Gråskala: en kanal
\end_layout

\end_deeper
\begin_layout Itemize
Spatial autokorrelation
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
2D-faltning av bild
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Visualisering: 
\begin_inset CommandInset href
LatexCommand href
name "här"
target "https://ezyang.github.io/convolution-visualizer/"
literal "false"

\end_inset


\end_layout

\begin_layout Itemize
För mer förklaringar och räkneexempel: 
\begin_inset CommandInset href
LatexCommand href
name "här"
target "https://towardsdatascience.com/visualizing-the-fundamentals-of-convolutional-neural-networks-6021e5b07f69"
literal "false"

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
2D-faltning av bild
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Graphics
	filename figs/2-D convolution.png
	scale 20

\end_inset


\begin_inset Newline newline
\end_inset


\size scriptsize
Från: Deep Learning, Ian Goodfellow and Yoshua Bengio and Aaron Courville,
 2016, 
\begin_inset CommandInset href
LatexCommand href
name "länk"
target "https://www.deeplearningbook.org/"
literal "false"

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Faltade lager
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Genomför faltning enligt föregående bild
\end_layout

\begin_deeper
\begin_layout Itemize
Nu erhålls en ny matris (bild)
\end_layout

\end_deeper
\begin_layout Enumerate
Aktiveringsfunktion appliceras elementvis (detector stage), ofta ReLu.
\end_layout

\begin_layout Enumerate
Pooling: 
\end_layout

\begin_deeper
\begin_layout Itemize
Tar ett rektangulärt område av bilden och beräknar sammanfattade mått 
\end_layout

\begin_deeper
\begin_layout Itemize
Medelvärde, maxvärde
\end_layout

\end_deeper
\begin_layout Itemize
Gör resultatet från faltningen invariant till små förändingar i input data
\end_layout

\end_deeper
\begin_layout Itemize
Notera: vi vill oftast ha flera filter per lager!
\end_layout

\begin_deeper
\begin_layout Itemize

\family typewriter
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = "relu", input_sha
pe = c(32,32,3))
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Pooling
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Pooling används ofta för varje (eller varanan) faltningslager 
\end_layout

\begin_layout Itemize
Hjälper till att reducera storleken på bilden (input) 
\begin_inset Formula $\rightarrow$
\end_inset

 variabelselektion
\end_layout

\begin_deeper
\begin_layout Itemize
Detta gör att senare lager inte behöver vara så stora
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Padding och Stride 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Hur hantera kanter på bilder?
\end_layout

\begin_deeper
\begin_layout Itemize
Om vi vill att input och output ska ha samma storlek:
\end_layout

\begin_layout Itemize

\series bold
Padding
\series default
: lägg ett eller flera lager med 0 (oftast) runt bilden 
\end_layout

\end_deeper
\begin_layout Itemize
Om vi vill spara beräkningar/parametrar:
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Stride
\series default
: Hoppa över pixlar när vi glider med kernelmatrisen över inputdata (i faltninge
n)
\end_layout

\begin_layout Itemize
Stride=2, hoppa över varanan pixel
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Padding
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Graphics
	filename figs/padding.png
	scale 21

\end_inset


\end_layout

\begin_layout Standard

\size scriptsize
Från: Deep Learning, Ian Goodfellow and Yoshua Bengio and Aaron Courville,
 2016, 
\begin_inset CommandInset href
LatexCommand href
name "länk"
target "https://www.deeplearningbook.org/"
literal "false"

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Faltning + aktiveringsfunktion + pooling
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Graphics
	filename figs/typical convolutional neural network layer.png
	scale 22

\end_inset


\end_layout

\begin_layout Standard

\size scriptsize
Från: Deep Learning, Ian Goodfellow and Yoshua Bengio and Aaron Courville,
 2016, 
\begin_inset CommandInset href
LatexCommand href
name "länk"
target "https://www.deeplearningbook.org/"
literal "false"

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Faltning + bilder = sant
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Generellt: Det är svårt att att få ut bra variabler/features från bilder
 
\end_layout

\begin_layout Itemize
Faltning är lämplig metod!
\end_layout

\begin_deeper
\begin_layout Itemize
Relativt få parametrar
\end_layout

\begin_layout Itemize
Använda lokal spatial information
\end_layout

\begin_layout Itemize
Spara beräkningar
\end_layout

\end_deeper
\begin_layout Itemize
Vilka filter ska vi välja?
\end_layout

\begin_deeper
\begin_layout Itemize
Neurala nätverk to the rescue!
\end_layout

\begin_layout Itemize
Vi lär oss alla parametrarna i ett eller flera filter med SGD
\end_layout

\end_deeper
\begin_layout Itemize
Förr: valdes filter 
\begin_inset Quotes eld
\end_inset

för hand
\begin_inset Quotes erd
\end_inset

 för att passa olika problem 
\begin_inset Formula $\rightarrow$
\end_inset

 ofta svårt!
\end_layout

\end_deeper
\begin_layout Frame

\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
CNN ARCHITECTURE - CLASSIFICATION
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Graphics
	filename figs/CNN ARCHITECTURE.png
	scale 25

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Avslut
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Frågor? Kommentarer?
\end_layout

\begin_layout Itemize
Kurshemsidan
\end_layout

\begin_layout Itemize
Labben
\end_layout

\end_deeper
\begin_layout Frame

\end_layout

\end_body
\end_document
