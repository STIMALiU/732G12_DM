\documentclass[10pt,english]{beamer}
%\documentclass[english,handout]{beamer} % For handouts
\input{../metropolis_preamble.tex}
\input{../macros.tex}
%\usepackage{extendedalt}
%\usepackage{animate} % Animations
%\usepackage{../lindsten}
%\usepackage{movie15}
\usepackage{tikz}
\usepackage{listofitems} % for \readlist to create arrays

\hypersetup{
  colorlinks=true, urlcolor=blue, linkcolor=red
}


\title{732G12 Data Mining}
\subtitle{Föreläsning 8}
\date{}
\author{Josef Wilzén \\ IDA, Linköping University, Sweden}
\titlegraphic{\hfill\includegraphics[height=1.2cm]{../LiU_primary_black.pdf}}
%\institute{Joint work with\dots}


%% MY DEF %%
\newcommand{\itm}[1]{\mathrm{Item}_{#1}}
\newcommand{\pausa}{\pause}
%\renewcommand{\pausa}{}
\tikzstyle{mynode}=[thick,draw=blue,fill=blue!20,circle,minimum size=22]


\newenvironment{nscenter}
 {\parskip=0pt\par\nopagebreak\centering}
 {\par\noindent\ignorespacesafterend}

\begin{document}

\maketitle

\begin{frame}{Dagens föreläsning}
    Agenda:
    \begin{itemize}
        \item Optimering av neurala nätverk
        \item Regularisering
        \item Faltade nätverk
        \item Autoencoder
    \end{itemize}

    \url{http://playground.tensorflow.org}

    Lek med olika modeller, se vad som händer etc.
    
    Projekt:
    \begin{itemize}
        \item Skapa grupper denna vecka
        \item Datainlämning: instruktioner kommer senare i veckan $\rightarrow$ kolla Teams
        \item Deadline datainlämning: torsdag kursvecka 6.
        \item Projektet fortsätter sen kursvecka 7.
    \end{itemize}
    
    
\end{frame}

\begin{frame}{Optimering av neurala nätverk}
    
    Gradient decent: Hitta minimum på en funktion genom att gå dit den lutar mest!

    Vill hitta
    \begin{equation*}
        a^* = \argmin_a L(a) = \sum_i L_i(f(x^{(i)},a), y^{(i)}).
    \end{equation*}

    Löser detta genom sekvensen
    \begin{equation*}
        a_{n+1} = a_n - \gamma \cdot \nabla L(a_n).
    \end{equation*}

    \begin{itemize}
        \item Vi behöver gradienter (partiella derivator)
        \item Backpropagation: kedjeregel för derivator på neruala nätverk.
        \item Gradient decent: dyrt när vi har många observationer!
    \end{itemize}

\end{frame}

\begin{frame}{Stochastic gradient decent (SGD)}

    Det är dyrt att beräkna $\nabla L(a_n)$ för alla datapunkter.

    Gör istället en väntesvärdesriktig skattning $\nabla \hat{L}(a_n)$ av gradienten genom att ta ett slumpmässigt sample från data (mini-batch).

    \begin{itemize}
        \item Större batch ger mindre varians i skattningen men blir dyrare att beräkna.
        \item Kräver fler iterationer och mindre learning rate.
        \item Kräver att vi har oberoende observationer.
        \item Funkar bra för neurala nätverk!
        \item En epok är en genomgång av all träningsdata.
    \end{itemize}
    
\end{frame}

\begin{frame}{Stochastic gradient decent (SGD)}
    
    Learning rate $\gamma$:
    \begin{itemize}
        \item Viktig parameter för att få konvergens.
        \item Behöver ofta ändras med antalet iterationer.
        \begin{itemize}
            \item Schema ger hur learning rate minskar: $\gamma_1, \gamma_2, \gamma_3, \ldots$
            \item Vanliga krav på schemat:
            \begin{equation*}
                \sum_{k=1}^{\infty} \gamma_k = \infty \qquad \sum_{k=1}^{\infty} \gamma_k^2 < \infty.
            \end{equation*}
            \item Exempel $\gamma_k = 1/k$.
        \end{itemize}
    \end{itemize}

\end{frame}

\begin{frame}{Stochastic gradient decent (SGD)}
    
    \myheading{Problem} I backpropagation multiplicerar vi massa derivator med varandra. Detta kan leda till försvinnande eller exploderande gradienter.
    \begin{itemize}
        \item Om derivatorna är nära noll så blir produkten noll.
        \item Om derivatorna är för stora blir produkten ännu större.
        \item ReLu fungerar ofta bättre än sigmoid. Finns olika sätt att hantera detta problem på.
    \end{itemize}

\end{frame}

\begin{frame}{Momentum}
    \begin{itemize}
        \item SGD är ofta bra, men ibland för långsamt.
        \item Idé: Kolla på hur gradient har varit i tidigare steg och använd detta för att ge skjuts åt algoritmen.
        \item Kallas för momentum.
        \item Hyperparameter $\alpha$, hur mycket av den gamla derivatan man sparar.
        \item Uppdatera parametrarna med $v$ som uppdateras som
        \begin{equation*}
            v = \alpha \cdot v - \gamma_k \cdot \widehat{\nabla L}
        \end{equation*}
        Vilket ger:
        \begin{equation*}
            \theta_{k} = \theta_{k-1} + v.
        \end{equation*}
    \end{itemize}
\end{frame}

\begin{frame}{Momentum}
    Kolla på \url{https://distill.pub/2017/momentum/} för visualisering

    \includegraphics[width=0.8\textwidth]{figs/Momentum.png}

    {\small{Från: Deep Learning, Ian Goodfellow and Yoshua Bengio and Aaron Courville, 2016}}
\end{frame}

\begin{frame}{Startvärden}
    
    Neurala nätverk har tusentals parametrar som vi måste skatta.
    \begin{itemize}
        \item SGD är iterativt, vi börjar någonstans och ändrar dessa startvärden.
        \item Dåliga startvärden kan leda till problem.
    \end{itemize}
    Vad kan vi göra:
    \begin{itemize}
        \item Bryta symmetri mellan noderna, vikter mellan lager kan inte ha exakt samma värden.
        \item Initiera vikterna slumpmässigt
        \begin{itemize}
            \item Uniformt
            \item Normalfördelat
        \end{itemize}
        \item Bias sätts för det mesta till något värde.
        \item Att sätta startparametrar kan ses som en hyperparameter.
        \item Keras dok: se \href{https://keras3.posit.co/reference/index.html\#initializers}{här}
    \end{itemize}

\end{frame}

\begin{frame}{Adaptiv inlärningstakt}

    Kan vara svårt att få till en bra learning rate som passar alla parametrar när vi använder SGD. En idé är att anpassa learning rate för varje parameter.
    \begin{itemize}
        \item RMSProp
        \begin{itemize}
            \item Skala om gradienten beroende på träningshistorik så att vi
            \begin{itemize}
                \item tar större steg för de parametrar med små derivator.
                \item tar mindre steg för de parametrar med stora derivator.
                \item Skala med exponentiellt avtagande glidande medelvärden för tidigare gradienter.
            \end{itemize}
        \end{itemize}
        \item Adam
        \begin{itemize}
            \item Kan ses som en blandning av RMSProp och momentum.
        \end{itemize}
        \item Finns fler varianter, RMSProp och Adam är vanliga standardval för neurala nätverk och finns i de flesta paket.
        \item Finns hyperparametrar.
    \end{itemize}
    
\end{frame}

\begin{frame}{Adam och RMSProp}

    \includegraphics[width = \textwidth]{figs/DL_training_curves.png}
    
\end{frame}


\begin{frame}{Batch normalization}

\href{https://en.wikipedia.org/wiki/Batch_normalization}{Batch normalization}:
  \begin{itemize}
    \item Metod för att göra träningen av neurala nätverk stabilare och snabbare.
    \item Funkar bra empiriskt, teorin om varför det funkar är inte klarlagd 
    \item Idén är standardisera alla inputs till ett lager
    \item Anta att vi använder någon variariant av SGD
    \begin{itemize}
      \item standardiseringen sker per nod i det aktuella lagret
      \item standardiseringen sker över den aktuella mini-batchen i ett träningssteg
      \item vi beräknar medelvärde och standardavvikelse baserat på aktuell mini-batch
    \end{itemize}
    
  \end{itemize}
   
    
\end{frame}


\begin{frame}{Batch normalization}

Låt $z_i$ vara ett set av noder i ett lager. Beräkna medelvärde och varians per nod över aktuell mini-batch:

$\mu_i = \frac{1}{m} \sum_{j=1}^m x_{ij} \quad$ och $\quad \sigma_{i}^2 = \frac{1}{m} \sum_{j=1}^m (x_{ij}-\mu_i)^2$

Normaliserar: 

$z_{i,norm}=\frac{z_i-\mu_i}{\sqrt{ \sigma_{i}^2 + \epsilon}}$

Skala om $z_{i,norm}$ med parametarar:

$\tilde{z_{i}} = \gamma_i \cdot z_{i,norm} + \beta_i$

Applicera aktiveringsfunktionen på $\tilde{z_{i}}$. 

$\gamma_i$ och  $\beta_i$ är parametarar som vi skattar tillsammans med alla parametrar i nätverket. 

Vid prediktioner så används medelvärde och varians beräknat på hela datasetet.

   
    
\end{frame}



\begin{frame}{Regularisering: krympning}
    Vad händer med kostandsfunktionen när vi kör Ridge/Lasso?
    
    Vad händer med parametrarna i modellen när vi kör Ridge/Lasso?

\end{frame}

\begin{frame}{Regularisering genom normstraff}
    
    $\text{Ridge} = \text{OLS} + \ell^2$-norm på $\beta$:
    \begin{equation*}
        \argmin_{\beta} L(\beta) = \sum_{i=1}^{n}(y_i - \hat{y})^2 + \lambda \sum_{j=1}^{p} \beta_{j}^2, \qquad \lambda \geq 0.
    \end{equation*}


    $\text{Lasso} = \text{OLS} + \ell^1$-norm på $\beta$:
    \begin{equation*}
        \argmin_{\beta} L(\beta) = \sum_{i=1}^{n}(y_i - \hat{y})^2 + \lambda \sum_{j=1}^{p} |\beta_{j}|, \qquad \lambda \geq 0.
    \end{equation*}
\end{frame}

\begin{frame}{Regularisering genom normstraff}
    
    Vi kan göra samma sak med neurala nätverk.

    Regularisering "tvingar" parametrarna att bli små (nära noll) vilket skapar en enklare funktion och minskar överanpassning.

    Vilken norm ska vi använda oss av?

\end{frame}

\begin{frame}{Regularisering genom normstraff}
    
    Generellt:
    \begin{equation*}
        \tilde{L}(\theta) = L(\theta) + \lambda \Omega(\theta),
    \end{equation*}
    \begin{description}
        \item[$L(\theta)$] kostandsfunktionen
        \item[$\Omega(\theta)$] straffterm
        \item[$\lambda$] Hyperparameter med $\lambda \geq 0$.   
    \end{description}

    Vad händer med  $\tilde{L}(\theta)$ då vi ändrar $\lambda$?

\end{frame}

\begin{frame}{Neurala nätverk med normstraff}
    \begin{itemize}
        \item Använd Frobenius normen:
        \begin{equation*}
            \|\mathbf{A}\|_{F}^2 = \sum_{i=1}^{q}\sum_{j=1}^{p} a_{ij}^2 \qquad \|\mathbf{A}\|^1_{F} = \sum_{i=1}^{q}\sum_{j=1}^{p} |a_{ij}|
        \end{equation*}
        där $\mathbf{A}$ är en $p \times q$ matris.
        \item Låt $\mathbf{W}_l$ vara viktmatrisen mellan lager $l$ och $l+1$, då får vi kostnadsfunktioner
        \begin{align*}
            \tilde{L}(\theta) &= L(\theta) + \frac{\lambda}{2} \sum_{l=1}^{h} \| \mathbf{W}_l \|_{F}^{2} \\
            \tilde{L}(\theta) &= L(\theta) + \frac{\lambda}{2} \sum_{l=1}^{h} \| \mathbf{W}_l \|_{F}^{1}
        \end{align*}
    \end{itemize}
\end{frame}

\begin{frame}{Neruala nätverk med normstraff}
    \begin{itemize}
        \item Vi kan ha olika $\lambda$ för olika lager,
        \begin{equation*}
            \frac{1}{2} \sum_{l=1}^h \lambda_l \|\mathbf{W}_l\|^2_F.
        \end{equation*}
        \item Kallas ibland för weight decay (tvingar vikterna att bli mindre).
    \end{itemize}
\end{frame}

\begin{frame}{Dropout}

    \begin{itemize}
        \item I varje iteration när vi tränar vår modell med SGD:
        \begin{itemize}
            \item Låt varje nod vara noll med en viss sannolikhet.
        \end{itemize}
        \item Nätverket kan inte "lita på" några specifika kopplingar.
        \item Nätverket måste distribuera sin kapacitet över hela nätverket, motverkar överanpassning.
        \item När värdena på vikterna sprids ut över många kopplingar så blir de mindre $\rightarrow$ Liknande effekt som vid normstraff.
        \item Dropout används vid träning, inte testning.
        \item Notera likheten med Random forest.
    \end{itemize}
    
\end{frame}

\begin{frame}{Dropout}
    \includegraphics[width=\textwidth]{figs/dropout_example.png}
\end{frame}

\begin{frame}{Mer Regularisering}
    Det finns flera andra sätt att regularisera
    \begin{itemize}
        \item Batch normalization: ger viss regularisering
        \item Skaffa mer träningsdata!
        \item Stoppa tidigt, använd valideringsdata för att avgöra när det är dags att stanna skattningen.
        \item Förstärk data (dataset augmentation), skapa mer artificiell data att träna på.
        \begin{itemize}
            \item Vanligt när det kommer till bilder.
            \item Spegla, rotera, beskära, addera brus m.m.
        \end{itemize}
        \item Ensamblemetoder
        \begin{itemize}
            \item Generera fler uppsättningar med startvärden och träna flera modeller: aggregera resultatet vid prediktioner.
        \end{itemize}
        \item Parameter Tying och Parameter Sharing
        \begin{itemize}
            \item Ha en struktur på modellen som tvingar vissa kopplingar att vara identiska eller glesa m.m.
            \item T.ex. CNN, RNN
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Speciella typer av modeller}
    \begin{itemize}
        \item Residual nets: Kopplingar som hoppar över ett eller flera lager.
        \begin{itemize}
            \item Skip layer connections.
            \item Tänk: "Modellera residualerna på en linjär modell med ett neuralt nätverk".
        \end{itemize}
        \item Transfer learning:
        \begin{itemize}
            \item "Överför kunskap mellan olika problem/uppgifter".
            \item Finns olika sorter, vanlig variant: Använd förtränade nätverk på andra problem.
            \begin{itemize}
                \item Ta ett neuralt nätverk som redan är anpassat för ett problem/dataset.
                \item Starta optimering för ett annat dataset med start i de redan tränade parametrarna.
                \item Kan ses som ett avancerat sätt att välja startvärden.
                \item Ofta effektivt på mindre dataset.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Faltade nätverk}
    Faltade nätverk, benämns ofta CNN (Convolutional neural networks)
    \begin{itemize}
        \item Speciella nätverk som passar för data som har en "rutnätsstruktur" (grid):
        \begin{itemize}
            \item 1-D nät: tidsserier
            \item 2-D när: bilder
            \item Tänk pixlar organiserade i en matris.
            \item \myheading{Används mycket framgångsrikt!}
            \item \href{https://dataconomy.com/2017/04/history-neural-networks/}{Lite historia}
        \end{itemize}
        \item Använder faltning mellan (minst två) lager i nätverket istället för vanliga matrismultiplikation.
    \end{itemize}
\end{frame}

\begin{frame}{Faltning}

    \begin{itemize}
        \item Faltning är en speciell typ av linjär operator.
        \item Kan defineras på olika sätt, kontinuerliga funktioner:
        \begin{align*}
            y_t &= \int x_{\tau} h(t - \tau) \mathrm{d}\tau = \int x(t-\tau) h(\tau) \mathrm{d}\tau \\
            y(t) &= (x * h) (t)
        \end{align*}
        \item Kan ses som en sammanviktning av två kontinuerliga funktioner
        \begin{itemize}
            \item $x(t)$ är vår insignal (input).
            \item $h(t)$ är vår kärnfunktion (kernel) eller filter.
            \item $y(t)$ är vår utsignal, kan kallas feature map i DL.
        \end{itemize}
    \end{itemize}
    
\end{frame}

\begin{frame}{Faltning}

    Faltninig är en vanlig operation inom statistik, matematik, signalbehandling, reglerteknik m.m.
    \begin{itemize}
        \item Används för att filtrera signaler/tidsserier, t.ex. för att ta bort brus.
        \item Används när vi vill beräkna täthetsfunktionen för \href{https://en.wikipedia.org/wiki/Convolution_of_probability_distributions}{summan av två oberoende slumpvariabler}.
        \begin{itemize}
            \item Givet $p(X), p(Y)$, låt $Z = X + Y$, hur beräkna $p(Z)$?
        \end{itemize}
    \end{itemize}
    
\end{frame}

\begin{frame}{Faltning}

    \begin{itemize}
        \item Om vi har diskret tid
        \begin{equation*}
            y(t) = (x * h) (t) = \sum_{\tau = - \infty}^{\infty} x(\tau) h(t - \tau),
        \end{equation*}
        $h$ blir här en vektor med värden (observera index i vektorn).
    \end{itemize}
    
\end{frame}

\begin{frame}{Faltning}

    \begin{itemize}
        \item I två dimensioner har vi
        \begin{equation*}
            y(i,j) = (x * h)(i,j) = \sum_m \sum_n x(m,n) h(i-m,j-n).
        \end{equation*}
        \item Kan generaliseras till flera dimensioner.
        \item Vi kan använda faltning som ett sätt att koppla ihop två lager i vårt neurala nätverk.
    \end{itemize}
    
\end{frame}

\begin{frame}{Varför faltning?}

    \begin{itemize}
        \item Faltning ger glesa kopplingar.
        \item Spatial information: närliggande pixlar relaterar till varandra mer än pixlar långt borta.
    \end{itemize}

    \includegraphics[width=.9\textwidth]{figs/CNN_sparse_con.png}
    
\end{frame}

\begin{frame}{Exempel: Klassificering av bilder: 2D}

    \begin{itemize}
        \item ImageNEt dataset / challenge. 1.2 miljoner bilder som träningsdata, 1000 klasser.
        \item \url{http://www.image-net.org/}
        \includegraphics[width = .8\textwidth]{figs/imageNet.png}
        \item \url{https://devopedia.org/imagenet}
    \end{itemize}
    
\end{frame}

\begin{frame}{Exempel: Klassificering av bilder: 2D}

    \includegraphics[width = \textwidth]{figs/Chexnet.png}
    
\end{frame}

\begin{frame}{Vad är en bild?}

    \begin{itemize}
        \item Bild: En 2D signal, varje pixel är ljusstyrka.
        \item Varje pixel har 2D koordinater $(x,y)$
        \item Varje pixel kan ha flera olika värden.
        \begin{itemize}
            \item Färgbiler, t.ex. röd grön och blå ger tre kanaler (channels).
            \item Gråskala, en kanal.
        \end{itemize}
        \item Spatial autokorrelation.
    \end{itemize}
    
\end{frame}

\begin{frame}{2D-faltning av bild}
    \href{https://ezyang.github.io/convolution-visualizer/}{Visualisering} \href{https://towardsdatascience.com/visualizing-the-fundamentals-of-convolutional-neural-networks-6021e5b07f69}{Förklaringar och räkneexempel}
    \includegraphics[width = 0.8\textwidth]{figs/2-D convolution.png}
\end{frame}

\begin{frame}{Faltade lager}
    \begin{itemize}
        \item Genomför faltning enligt föregående bild.
        \begin{itemize}
            \item Vi erhåller nu en ny matris (bild).
        \end{itemize}
        \item Aktiveringsfunktionen appliceras elementvis.
        \item Pooling:
        \begin{itemize}
            \item Tar ett rektangulärt område av bilden och beräkna sammanfattade mått.
            \item Gör resultatet från faltning invariant mot små förändringar i input data.
        \end{itemize}
        \item Notera att vi ofta vill ha fler filter per lager.
    \end{itemize}
\end{frame}

\begin{frame}{Pooling}
    
    \begin{itemize}
        \item Pooling används ofta för varje (eller vartannat) faltningslager.
        \item Hjälper till att reducera storleken på bilden.
        \begin{itemize}
            \item Betyder att senare lager inte behöver vara så stora.
        \end{itemize}
    \end{itemize}

\end{frame}

\begin{frame}{Padding och Stride}
    
    \begin{itemize}
        \item Vid faltning av bild blir resultatet mindre.
        \begin{itemize}
            \item \myheading{Padding} lägg till ett eller flera lager med 0 runt bilden.
            \item Rätt padding gör att output har samma storlek som input.
        \end{itemize}
        \item Om vi vill spara beräkningar/parametrar.
        \begin{itemize}
            \item \myheading{Stride} Hoppas över pixlar när vi glider med kernelmatrisen i faltningen.
            \item Stride = 2, hoppar över varannan pixel.
        \end{itemize}
    \end{itemize}

\end{frame}

\begin{frame}{Padding}
    
    \includegraphics[height=\textheight]{figs/padding.png}

\end{frame}

\begin{frame}{Ett komplett lager}

    \includegraphics[height=\textheight]{figs/typical convolutional neural network layer.png}
    
\end{frame}

\begin{frame}{Sammanfattning}

    \begin{itemize}
        \item Generellt: Det är svårt att få ut bra variabler/features från bilder.
        \item Faltning är en lämplig metod!
        \begin{itemize}
            \item Relativt få parametrar.
            \item Använder lokal spatial information.
            \item Sparar beräkningar.
        \end{itemize}
        \item Vilka filter ska vi välja?
        \begin{itemize}
            \item $\rightarrow$ Neurala nätverk till undsättning!
            \item Vi lär oss parametrarna i ett eller flera filter med SGD (RMSProp, Adam etc.)
        \end{itemize}
        \item Förr valdes filter "för hand" för att passa olika problem.
    \end{itemize}
    
\end{frame}

\begin{frame}{CNN Arkitektur för Klassificering}
    \includegraphics[width = \textwidth]{figs/CNN ARCHITECTURE.png}
\end{frame}


\begin{frame}{Autoencoder}
    
    \href{https://en.wikipedia.org/wiki/Autoencoder}{Autoencoder}:
    \begin{itemize}
      \item Målet är att lära sig en effektiv kod/representation av en dataset $X$.
      \item Exempel på \textbf{oövervakad inlärning}
      \item Oftast så har koden/representation lägre dimension än $X$.
      \item Nätverk med två delar: encoder och decoder
    \end{itemize}
        
    \begin{equation*}
        X \rightarrow encoder \rightarrow Z/code \rightarrow decoder \rightarrow X'
    \end{equation*}

    \includegraphics[scale=0.32]{figs/Autoencoder_schema.png}
    
\end{frame}

\begin{frame}{Autoencoder}
    
    Vanligt att använda djupa nätverk: encodern och decodern har noder i flera lager.
    
    Antalet noder brukar minskas successivt tills kod-lagret, för sen successivt ökas till det sista lagret. Antalet noder i kod-lagret är en viktig hyperparameter.
    
    Input och output är lika stora.
    
    \includegraphics[scale=0.30]{figs/Autoencoder_structure_deep.png}
    

\end{frame}


\begin{frame}{Autoencoder: Träning}
    
    \begin{itemize}
      \item Encoder: $E_\phi: X \rightarrow Z$
      \item Decoder: $D_\theta: Z \rightarrow X$
      \item Anpassade värden: $X' = D_\theta (E_\phi(X))$
      \item $D_\phi()$ och $E_\theta()$: kan vara MLP, CNN etc
    \end{itemize}
        
    Låt $d(x,x')$ vara ett avståndsmått, kostnadsfunktion:
    
    \begin{equation*}
        L(\theta ,\phi) = \mathbb{E}[d(x,x')] = \mathbb{E}[d(x,D_\theta (E_\phi(X)))]
    \end{equation*}
    
    Om vi använder Euklidiskt avstånd och MSE som kostnadsfunktion:
    
    \begin{equation*}
        \argmin_{\theta ,\phi} L(\theta ,\phi) = \frac{1}{N} \sum_{i=1}^{N} \| x_i - D_\theta(E_\phi(X)) \|_{2}^{2}
    \end{equation*}
    
\end{frame}


\begin{frame}{Autoencoder}
    
    Finns olika varaianter
    \begin{itemize}
      \item Icke-linjär variabelreduktion/transformation
      \begin{itemize}
        \item Notera: Om vi har en linjär autoencoder så är det likt PCA
      \end{itemize}
      \item Sparse autoencoder: koden tvingas att vara nära noll för de flesta noder
      \item Denoising autoencoder - brusreducering: tränar på ett $X$ där man lagt till brus och försöker återskapa $X$ utan brus
      \item Variational autoencoder (VAE): en generativ modell där koden som man lär sig är en sannolikhetsfördelning $\rightarrow$ lär sig att sampla från fördelningen för $X$
    \end{itemize}
    
\end{frame} 
     

\begin{frame}{Autoencoder: exempel}
    
    
    \includegraphics[scale=0.32]{figs/Reconstruction_autoencoders_vs_PCA.png}
    
    ``Reconstruction of 28x28pixel images by an Autoencoder with a code size of two (two-units hidden layer) and the reconstruction from the first two Principal Components of PCA. Images come from the Fashion MNIST dataset.''  - \href{https://en.wikipedia.org/wiki/Autoencoder\#Principal_component_analysis}{källa}

\end{frame}


\end{document}