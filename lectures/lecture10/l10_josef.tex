\documentclass[10pt,english]{beamer}
%\documentclass[english,handout]{beamer} % For handouts
\input{../metropolis_preamble.tex}
\input{../macros.tex}
%\usepackage{extendedalt}
%\usepackage{animate} % Animations
%\usepackage{../lindsten}
%\usepackage{movie15}
\usepackage{tikz}
\usepackage{listofitems} % for \readlist to create arrays

\title{732G12 Data Mining}
\subtitle{Föreläsning 10}
\date{}
\author{Josef Wilzén \\ IDA, Linköping University, Sweden}
\titlegraphic{\hfill\includegraphics[height=1.2cm]{../LiU_primary_black.pdf}}
%\institute{Joint work with\dots}


%% MY DEF %%
\newcommand{\itm}[1]{\mathrm{Item}_{#1}}
\newcommand{\pausa}{\pause}
%\renewcommand{\pausa}{}
\tikzstyle{mynode}=[thick,draw=blue,fill=blue!20,circle,minimum size=22]


\newenvironment{nscenter}
 {\parskip=0pt\par\nopagebreak\centering}
 {\par\noindent\ignorespacesafterend}

\begin{document}

\maketitle

\begin{frame}{Dagens föreläsning}

    \begin{itemize}
        \item K-medoid klustring
        \item Densitetsbaserade metoder
        \item Faktorer som påverkar klusteranalys
        \item Utvärdera klusteranalys
    \end{itemize}
    
\end{frame}

\begin{frame}{Information Kandidatuppsats}
    
    Kommer (troligtvis) vara ett informationsmöte om kandidatuppsatsen på torsdag nästa vecka. 

\end{frame}

\begin{frame}{K-medoid klustring}

    Använder \imp{medoider} som center/prototyp vid klustring.
    \begin{itemize}
        \item En \imp{medoid} är en representativ observation inom ett dataset/kluster.
        \item Medoid är \imp{inte} samma som centroid, median, geometrisk median etc.
        \item Medoider är \imp{lätta att tolka}
        \begin{itemize}
            \item centroider kan vara punkter som inte liknar någon av observationerna i data.
        \end{itemize}
        \item k-medoids:
        \begin{itemize}
            \item minimerar summan av parvisa avstånd.
            \item kan använda godtyckligt avståndsmått.
            \item mer robust med brus och extremvärden.
        \end{itemize}
        \item k-means: använder oftast euklidiskt avstånd.
        \item k-medoid klustring kallas också Partitioning Around Medoids (PAM)
    \end{itemize}
    
\end{frame}

\begin{frame}{K-medoid klustring}

    \includegraphics[width = \textwidth]{figs/k-mediod.png}
    
\end{frame}



\begin{frame}{Densitetsbaserade metoder}

    Kluster kan formas baserat på hur densiteten på punkter varierar över variablerna: Täta områden kan defineras som ett kluster.

    \includegraphics[width = .8\textwidth]{figs/density_cluster2.png}
    
\end{frame}

\begin{frame}{DBSCAN}

    \begin{itemize}
        \item Algoritm för att skapa kluster baserat på punkternas täther.
        \item Använder två begrepp:
        \begin{itemize}
            \item $\operatorname{eps}$, en sökradie där vi letar efter punkter.
            \item $\operatorname{minPts}$, minsta antal punkter.
        \end{itemize}
    \end{itemize}
    
    Från detta kan vi klassa observation i någon av följande:
    \begin{description}
        \item[Kärnpunkt] Punkter med fler än $\operatorname{minPts}$ punkter inom sökradien $\operatorname{eps}$.
        \item[Gränspunkt] Inte en kärnpunkt men hamnar inom sökradien från en kärnpunkt.
        \item[Bruspunkt] Varken kärnpunkt eller gränspunkt.   
    \end{description}

\end{frame}

\begin{frame}{Illustration}
    
    \includegraphics[width=\textwidth]{figs/DBSCAN_obs_class.png}

\end{frame}

\begin{frame}{DBSCAN Algoritmen}

    \includegraphics[width=\textwidth]{figs/DBSCAN algoritm.png}
    
\end{frame}

\begin{frame}{Val av $\operatorname{eps}$ och $\operatorname{minPts}$}

    Hyperparametrar som vi måste välja.
    \begin{enumerate}
        \item Definera ett nummer $k$.
        \item Beräkna avståndet mellan varje punkt och dess $k$-närmaste granne och sortera punkterna enligt ökande avstånd.
        \item Definera $\operatorname{eps}$ som värdet där skarp förändring märks (armbågsmetoden).
        \item $\operatorname{minPts} = k$.
    \end{enumerate}
    $k$-värdet vi valt i steg 1 påverkar \imp{inte} $\operatorname{eps}$-värdet mycket om $k$ inte är extremt (för litet eller för stort).
    
\end{frame}

\begin{frame}{DBSCAN - Exempel}

    \includegraphics[width=\textwidth]{figs/density_cluster.png}
    
\end{frame}

\begin{frame}{DBSCAN - Exempel}
    
    \includegraphics[width=\textwidth]{figs/DBSCAN_cluster_example.png}

\end{frame}

\begin{frame}{DBSCAN - För och nackdelar}

    \begin{itemize}
        \item Brusbeständig.
        \item Behandlar kluster av olika former och storlekar.
        \item Problem med kluster som har betydligt varierande tätheter.
        \begin{itemize}
            \item Svårt att välja ett bra $\operatorname{eps}$.
        \end{itemize}
        \item Problem i stora dimensioner.
    \end{itemize}
    
\end{frame}

\begin{frame}{K-means och DBSCAN}

    \includegraphics[width=\textwidth]{figs/compre_k-mean_DBACAN.png}
    
\end{frame}

\begin{frame}{Faktorer som påverkar klusteranalys}
    \begin{itemize}
        \item Dimensionalitet (problem för täthetsbaserade metoder).
        \item Datamängdens storlek (stora datamängder är svåra att skala upp).
        \item Brus och extremvärden.
        \item Skalan på data: numerisk, kategorisk.
        \begin{itemize}
            \item problem att välja närhetsmått för datamängder med blandade attribut.
        \end{itemize}
        \item Standardisering av variabler.
    \end{itemize}
\end{frame}

\begin{frame}{Egenskaper}
    
\begin{itemize}
    \item Fördelningar - Olika metoder passar bättre på vissa fördelningar.
    \item Form - Godtyckliga former är svårare att klustra.
    \item Storlek - K-means, problem med olika storlekar.
    \item Täthet - Olika täthet problem för K-means, DBSCAN.
    \item Dåligt separerade kluster - Vissa metoder slår ihop överlappande kluster eller kluster som ligger nära varandra
\end{itemize}

\begin{redbox}
Ingen klustermetod passar för alla dataset!
\end{redbox}

\end{frame}

\begin{frame}{Utvärdera klusteranalys}

    \begin{itemize}
        \item Cluster tendency: Finns det kluster i data? Eller har observationerna bara slumpmässiga värden?
        \item Avgöra rätt antal kluster.
        \item Interna mått på hur bra klusteranalysen är.
        \item Externa mått på hur bra klusteranalysen är $\rightarrow$ om vi har tillgång till sanna klasser/grupper.
        \item Jämföra olika metoder för klusteranalys på samma dataset.
        \item Kontext och problembeskrivning, avgör om vi har en bra klustring.
    \end{itemize}
    
\end{frame}

\begin{frame}{Cohesion och Separation}
    Interna mått.
    \begin{description}
        \item[Cohesion:] Hur tight eller sammanhållet ett kluster är med sig själv.
        \item[Separation:] Hur väl separerat ett kluster är från övriga kluster.
    \end{description}
    
    När vi har beräknat mått för ett kluster kan vi väga samman alla dessa mått till ett mått.

\end{frame}

\begin{frame}{Cohesion och Separation}
    
    \begin{align*}
        \operatorname{cohesion}(C_i) &= \sum_{x \in C_i, y \in C_i} \operatorname{proximity}(x,y) \\
        \operatorname{proximity}(C_i, C_j) &= \sum_{x \in C_i, y \in C_j} \operatorname{proximity}(x,y)
    \end{align*}

    \includegraphics[width=.8\textwidth]{figs/cohesion and separation.1.png}

    $\operatorname{proximity}(x,y)$ kan vara både närhetsmått eller avståndsmått.

\end{frame}

\begin{frame}{The Silhouette Coefficient}
    
    Använder både cohesion och separation för att beräkna ett mått.

    \begin{enumerate}
        \item Beräkna medelavståndet från $\text{observation}_i$ till alla andra observationer i dess kluster, kalla det $a_i$.
        \item Beräkna nu medelavståndet från $\text{observation}_i$ till alla kluster som inte innehåller denna observation.
        \item Hitta det minsta av dessa avstånd kalla det $b_i$.
        \item Silhouette coefficient för $\text{observation}_i$ defineras som,
        \begin{equation*}
            s_i = \frac{b_i - a_i}{\max(a_i, b_i)}
        \end{equation*}
    \end{enumerate}

\end{frame}

\begin{frame}{The Silhouette Coefficient}

    \begin{equation*}
        s_i = \frac{b_i - a_i}{\max(a_i, b_i)}
    \end{equation*}

    \begin{itemize}
        \item $s_i$ kan ta värden mellan $-1$ och $1$.
        \item 1 är bästa möjliga värde.
        \begin{itemize}
            \item Vill ha $a_i < b_i$ och att $a_i$ ska vara nära noll.
        \end{itemize}
        \item Average silhouette coefficient.
        \begin{itemize}
            \item Ta medelvärdet över alla $s_i$
            \item Ger ett mått på hur bra klustringen är.
        \end{itemize}
    \end{itemize}
    
\end{frame}

\begin{frame}{The Silhouette Coefficient - Exempel}

    \includegraphics[width=\textwidth]{figs/Silhouette coefficients for points in ten clusters..png}
    
\end{frame}

\begin{frame}{Välja antal kluster}

    \begin{itemize}
        \item K-means: vi kan använda total SSE och average silhouette coefficient.
        \item Plotta dessa mot antal kluster.
        \begin{itemize}
            \item Kolla efter böjar och toppar.
            \item SSE planar ut efter en böj: ta antal kluster vid böjen.
            \item Average silhouette coefficient: Kolla om det finns en eller flera toppar.
        \end{itemize}
    \end{itemize}
    
\end{frame}

\begin{frame}{Välja antal kluster - Exempel}

    \includegraphics[width=\textwidth]{figs/Determining the Correct Number of Clusters.png}
    
\end{frame}

\begin{frame}{Calinski-Harabasz Index}
    \begin{description}
        \item[Inter-cluster dispersion]
        \begin{equation*}
            \operatorname{BGSS} = \sum_{k=1}^{K} n_k \| C_k - C \|^2.
        \end{equation*}
        \item[Intra-cluster dispersion]
        \begin{equation*}
            \operatorname{WGSS}_k = \sum_{i=1}^{n_k} \| x_{i,k} - C_k \|^2, \qquad \operatorname{WGSS} = \sum_{k=1}^{K} \operatorname{WGSS}_k.
        \end{equation*}
        \item[Calinski-Harabasz Index]
        \begin{equation*}
            \operatorname{CH} = \frac{\operatorname{BGSS}}{\operatorname{WGSS}} \cdot \frac{N - K}{K-1}.
        \end{equation*}
    \end{description}
    Höga värden är bra för CH.

    Davies-Bouldin Index är ett liknande mått. Där ska man ha låga värden.
\end{frame}

\begin{frame}{Välja antal kluster}

    \begin{itemize}
        \item Vi kan beräkna närhetsmatrisen eller avståndsmatrisen för alla datapunkter.
        \begin{itemize}
            \item Matris med alla parvisa närheter/avstånd mellan observationer.
        \end{itemize}
        \item Notera att detta är dyrt!
        \begin{itemize}
            \item Kostar $O(n^2)$
            \item Svårt att plotta med många observationer.
            \item En lösning är att ta ett slumpmässigt urval av data.
        \end{itemize}
        \item Sortera närhetsmatrisen baserat på kluster.
        \begin{itemize}
            \item Först kommer kluster 1, sen kluster 2, osv.
        \end{itemize}
        \item Om vi har väl separerade kluster och valt ett bra antal kluster kommer den sorterade matrisen vara ungefär blockdiagonal.
    \end{itemize}
    
\end{frame}

\begin{frame}{Välja antal kluster - Exempel}

    \includegraphics[width=.8\textwidth]{figs/Similarity matrices for clusters.png}
    
\end{frame}

\begin{frame}{Cluster Tendency}

    \begin{itemize}
        \item Har vi slumpmässig data eller finns det något mönster? (kluster)
        \item Sampla två grupper om $p$ datapunkter
        \begin{itemize}
            \item Uniformt fördelade från datarymden.
            \item Från datasetet utan återläggning.
        \end{itemize}
        \item Beräkna avståndet till närmaste granne i datasetet.
        \begin{itemize}
            \item $u_i$ är minsta avståndet från en uniform datapunkt till en observation.
            \item $w_i$ är minsta avståndet från en samplad datapunkt till en icke-samplad datapunkt.
        \end{itemize}
        \item Hopkins statistic:
        \begin{equation*}
            \operatorname{H} 0 \frac{\sum_{i=1}^{p}w_i}{\sum_{i=1}^{p}u_i + \sum_{i=1}^{p}w_i}
        \end{equation*}
        \item Nollhypotesen är att datasetet följer en uniform fördelning. $\operatorname{H_0}$ kommer då vara $\operatorname{Beta}(p,p)$ fördelat.
        \item Värden nära 1 indikerar att data inte är uniformt fördelat.
        
    \end{itemize}
    
\end{frame}

\begin{frame}{Extern validering}

    \begin{itemize}
        \item Jämför med sanna klasser/kluster.
        \item Vi kan ta resultatet från vår klusteranalys som våra "predikterade värden"
        \item Kan då jämföra med sanna klasserna.
        \begin{itemize}
            \item Vi kan då beräkna förväxlingsmatris och liknande mått.
        \end{itemize}
        \item Notera:
        \begin{itemize}
            \item Vi har inte de "rätta namnen" på våra kluster.
            \item Vi vill ofta att klustren ska vara så rena som möjligt, dvs. domineras av en klass.
        \end{itemize}
    \end{itemize}
    
\end{frame}

\end{document}