\documentclass[10pt,english]{beamer}
%\documentclass[english,handout]{beamer} % For handouts
\input{../metropolis_preamble.tex}
\input{../macros.tex}
%\usepackage{extendedalt}
%\usepackage{animate} % Animations
%\usepackage{../lindsten}
%\usepackage{movie15}

\title{732G12 Data Mining}
\subtitle{Föreläsning 4}
\date{}
\author{Johan Alenlöv \\ IDA, Linköping University, Sweden}
\titlegraphic{\hfill\includegraphics[height=1.2cm]{../LiU_primary_black.pdf}}
%\institute{Joint work with\dots}


%% MY DEF %%
\newcommand{\itm}[1]{\mathrm{Item}_{#1}}
\newcommand{\pausa}{\pause}
%\renewcommand{\pausa}{}


\newenvironment{nscenter}
 {\parskip=0pt\par\nopagebreak\centering}
 {\par\noindent\ignorespacesafterend}

\begin{document}

\maketitle

\begin{frame}{Dagens föreläsning}

    \begin{itemize}
        \item K-närmaste grannar
        \item Bayesianska klassificerare
        \item Ensamblemetoder
        \begin{itemize}
            \item Bagging
            \item Boosting
            \item Random forest
        \end{itemize}
    \end{itemize}
    
\end{frame}


\begin{frame}{K-närmaste grannar}
    \begin{greenbox}
        \myheading{Idé} basera predikation på de K datapunkter som är närmast.
   \end{greenbox}

   Ger en icke-parametrisk metod för klassificering och regression.

   Problem: Vad är närmast?
\end{frame}

\begin{frame}{Avståndsmått}
    Vi behöver något som talar om för oss hur nära två datapunkter är. Finns många alternativ som man kan välja, som ger olika resultat.

    \begin{description}
        \item[Euklidiskt avstånd]
        \begin{equation*}
            d(\mathbf{x},\mathbf{y}) = \sqrt{\sum_{k=1}^{n} (x_k - y_k)^2}
        \end{equation*}
        \item[Manhattan avstånd] 
        \begin{equation*}
            d(\mathbf{x},\mathbf{y}) = \sum_{k=1}^{n} | x_k - y_k |
        \end{equation*}
    \end{description}
\end{frame}

\begin{frame}{K-närmaste grannar}
    \begin{enumerate}
        \item Låt $k$ vara ditt valda antal grannar och $D$ din träninigsdata.
        \item För varje testdata $z = (\mathbf{x}', y') \in D$:
        \begin{enumerate}
            \item Beräkna $d(\mathbf{x},\mathbf{x}')$ (avstådet mellan $z$ och all träningsdata)
            \item Välj $D_z \subseteq D$, de $k$ närmaste träniningsdatan till $z$
            \item Låt $y' = \argmax_v \sum_{(\mathbf{x}_i, y_i) \in D_z} \mathbf{I}_{v = y_i}$
        \end{enumerate}
    \end{enumerate}
    2.3 är majoritetsvalet. Kan också vikta detta värde med avståndet:
    \begin{itemize}
        \item[2.3] $y' = \argmax_{v} \sum_{(\mathbf{x}_i, y_i) \in D_z} w_i \mathbf{I}_{v = y_i}$. 
    \end{itemize}

    För regression används medelvärde alternativt viktat medelvärde.
\end{frame}

\begin{frame}{K-närmaste grannar}

    \includegraphics[width = \textwidth]{figs/KNN_fit_data.png}
    
\end{frame}

\begin{frame}{K-närmaste grannar}

    \begin{itemize}
        \item Målet med modellen är att prediktera nya observationer.
        \item Påverkas stort av olika skalor.
        \item Långsam anpassning.
        \item Känslig mot brus.
        \item Val av K har stor betydelse!
        \begin{itemize}
            \item Litet K ger överanpassning.
            \item Stort K ger underanpassning.
            \item Korsvalidering kan användas för att bestämma K.
        \end{itemize}
        \item Producerar godtyckligt utformade beslutsgränser.
        \item Problem i högre dimensioner.
    \end{itemize}
    
\end{frame}

\begin{frame}{Bayesiansk klassificerare}
    Att direkt modellera en icke-deterministisk funktion kan vara mycket svårt.

    Exempel:
    \begin{itemize}
        \item $(\text{diet}, \text{träning}) \to (\text{hjärtinfarkt})$ är svårt
        \item $(\text{diet}, \text{träning}) \to \mathbb{P}(\text{hjärtinfarkt})$ lättare
    \end{itemize}

    Använd Bayes sats för att hjälpa till i modelleringen
    \begin{align*}
        \mathbb{P}(Y \mid \mathbf{X}) &= \frac{\mathbb{P}(\mathbf{X} \mid Y)}{\mathbb{P}(\mathbf{X})} \cdot \mathbb{P}(Y) \propto \mathbb{P}(\mathbf{X} \mid Y) \cdot \mathbb{P}(Y) \\
        \text{posterior} &= \frac{\text{likelihood}}{\text{evidence}} \cdot \text{prior} \propto \text{likelihood} \cdot \text{prior}
    \end{align*}
\end{frame}

\begin{frame}{Kategoriska attribut}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            $\mathbb{P}(Y = y)$ är andelen datapunkter med klass $y$.

            $\mathbb{P}(X_i = x_i \mid Y = y)$ andelen datapunkter med attribut $x_i$ av datapunkterna med klass $y$.
        \end{column}
        \begin{column}{0.5\textwidth}
            \includegraphics[width=\textwidth]{figs/data_bayes_class.png}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Kontinuerliga attribut}

    För kontinuerliga attribut finns olika tillvägagångssätt.

    \begin{itemize}
        \item Diskretisera data i olika kategorier.
        \begin{itemize}
            \item För få intervall gör att man missar information.
            \item För många intervall kan ge intervall utan observationer.
        \end{itemize}
        \item Anta en sannolikhetsfördelning för variabeln och skatta parametrarna från träniningsdatan.
        \begin{itemize}
            \item Normalfördelningen är vanlig.
            \item Conjugate prior.
        \end{itemize}
    \end{itemize}
    
\end{frame}

\begin{frame}{Grundläggande princip}

    Träningfasen:

    Skatta sannolikheten $\mathbb{P}(Y \mid \mathbf{X})$ för alla möjliga $\mathbf{X}$ och $y$.

    Klassificeringsfas:

    Givet $\mathbf{X}'$ skatta klass $Y' = \max_Y \mathbf{P}(Y \mid \mathbf{X}')$.
    
\end{frame}

\begin{frame}{Naiv Bayes klassificerare}

    Modellantagande:
    \begin{equation*}
        \mathbb{P}(\mathbf{X} \mid Y) = \prod_i \mathbb{P}(X_i \mid Y),
    \end{equation*}
    alla $X_i$ är oberoende av varandra. Vi kan då faktorisera likelihooden över $\mathbf{X}$.

    Använder vi detta får vi en sannolikhet
    \begin{equation*}
        \mathbb{P}(Y \mid \mathbf{X}) = \prod_i \mathbb{P}(X_i \mid Y) \mathbb{Y},
    \end{equation*}
    det räcker med att skatta sannolikheten för varje $X_i$. Detta ger oss en enklare modell som går att skatta.
    
\end{frame}

\begin{frame}{Egenskaper hos naiv Bayes}
    \begin{itemize}
        \item Metoden är robust mot isolerade bruspunkter.
        \item Metoden är robust mot irrelevanta attribut.
        \item Lätt att skatta.
        \item Korrelerade attribut kan väsentligt försämra prestandan.
        \begin{itemize}
            \item Behöver en mer komplex modell för att hantera.
            \item Simultan sannolikhetsfördelning för likelihooden.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Ensamblemetoder}
    
    \includegraphics[width=.8\textwidth]{figs/ensample_cat.png}

\end{frame}

\end{document}